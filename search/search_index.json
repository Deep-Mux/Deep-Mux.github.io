{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Deep Mux Official website DeepMux is a platform to deploy machine learning models into production. Our system allows you to use only as much GPU time as you really need. We automatically pick the best hardware that suits your model. Simply upload your model and get predictions, zero tweaking required.","title":"Overview"},{"location":"#welcome-to-deep-mux","text":"Official website DeepMux is a platform to deploy machine learning models into production. Our system allows you to use only as much GPU time as you really need. We automatically pick the best hardware that suits your model. Simply upload your model and get predictions, zero tweaking required.","title":"Welcome to Deep Mux"},{"location":"details/","text":"Cold start Model inference time may increase after model has been idle for some time. It happens due to the fact that we need to load model into the memory before executing it. Increase in latency depends on the model size. E.g. for 200MB model it would take about 500ms to load. You can project loading time onto your model as it scales linearly. Time after which model is unloaded is not fixed and is subject to constant adjustment. Think tens of seconds to minutes. If the model is loaded when a request occurs there is 2-3 ms of latency per request added by the infrastructure. Billing You are charged only for inference time and not for any kind of infrastructure latency. Note that request times on the client may differ from the ones observed in monitoring due to network latency / throughput.","title":"Details"},{"location":"details/#cold-start","text":"Model inference time may increase after model has been idle for some time. It happens due to the fact that we need to load model into the memory before executing it. Increase in latency depends on the model size. E.g. for 200MB model it would take about 500ms to load. You can project loading time onto your model as it scales linearly. Time after which model is unloaded is not fixed and is subject to constant adjustment. Think tens of seconds to minutes. If the model is loaded when a request occurs there is 2-3 ms of latency per request added by the infrastructure.","title":"Cold start"},{"location":"details/#billing","text":"You are charged only for inference time and not for any kind of infrastructure latency. Note that request times on the client may differ from the ones observed in monitoring due to network latency / throughput.","title":"Billing"},{"location":"faq/","text":"FAQ","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"client-libraries/python/","text":"Python client library user guide deepmux is a PaaS solution to effortlessly deploy trained machine learning models on the cloud and generate predictions without setting up any hardware. At the moment only pytorch models are supported. Installation pip install deepmux==0.2.4 Quickstart Creating model Initialize a model and upload it to deepmux servers from deepmux import create_model model = create_model(<YOUR PYTORCH MODEL>, <MODEL_NAME>, <SHAPE OF INPUT DATA>, <SHAPE OF OUTPUT DATA>, <TOKEN>) Getting model by name On your production server you can simply get model by it's name. from deepmux import get_model model = get_model(<MODEL NAME>, <TOKEN>) Executing model on your import After initializing your model with create_model or get_model you can run the model. All computations will be performed on deepmux infrastucture. output = model.run(<YOUR INPUT>) Complete example on a model from PyTorch Hub import numpy as np import torch from deepmux import create_model token = \"<YOUR_TOKEN>\" pytorch_model = torch.hub.load('pytorch/vision:v0.5.0', 'squeezenet1_0', pretrained=True) deepmux_model = create_model( pytorch_model, model_name='my_model', input_shape=[1, 3, 227, 227], output_shape=[1, 1000], token=token) dummy_input = np.zeros([1, 3, 227, 227], dtype=np.float32) output = deepmux_model.run(dummy_input) Getting token Currently, deepmux is in closed testing. You can get your own token by contacting tna0y or alexsaplin . Colab example Try our demo","title":"Python"},{"location":"client-libraries/python/#python-client-library-user-guide","text":"deepmux is a PaaS solution to effortlessly deploy trained machine learning models on the cloud and generate predictions without setting up any hardware. At the moment only pytorch models are supported.","title":"Python client library user guide"},{"location":"client-libraries/python/#installation","text":"pip install deepmux==0.2.4","title":"Installation"},{"location":"client-libraries/python/#quickstart","text":"","title":"Quickstart"},{"location":"client-libraries/python/#creating-model","text":"Initialize a model and upload it to deepmux servers from deepmux import create_model model = create_model(<YOUR PYTORCH MODEL>, <MODEL_NAME>, <SHAPE OF INPUT DATA>, <SHAPE OF OUTPUT DATA>, <TOKEN>)","title":"Creating model"},{"location":"client-libraries/python/#getting-model-by-name","text":"On your production server you can simply get model by it's name. from deepmux import get_model model = get_model(<MODEL NAME>, <TOKEN>)","title":"Getting model by name"},{"location":"client-libraries/python/#executing-model-on-your-import","text":"After initializing your model with create_model or get_model you can run the model. All computations will be performed on deepmux infrastucture. output = model.run(<YOUR INPUT>)","title":"Executing model on your import"},{"location":"client-libraries/python/#complete-example-on-a-model-from-pytorch-hub","text":"import numpy as np import torch from deepmux import create_model token = \"<YOUR_TOKEN>\" pytorch_model = torch.hub.load('pytorch/vision:v0.5.0', 'squeezenet1_0', pretrained=True) deepmux_model = create_model( pytorch_model, model_name='my_model', input_shape=[1, 3, 227, 227], output_shape=[1, 1000], token=token) dummy_input = np.zeros([1, 3, 227, 227], dtype=np.float32) output = deepmux_model.run(dummy_input)","title":"Complete example on a model from PyTorch Hub"},{"location":"client-libraries/python/#getting-token","text":"Currently, deepmux is in closed testing. You can get your own token by contacting tna0y or alexsaplin .","title":"Getting token"},{"location":"client-libraries/python/#colab-example","text":"Try our demo","title":"Colab example"}]}